# CMCC-34 Dataset LLM-based Data Augmentation

## ğŸ“‹ Overview

This directory contains an improved **LLM-based data augmentation system** specifically designed to address severe class imbalance in the CMCC-34 dataset. The system uses GLM-4 model to generate high-quality synthetic samples for minority classes.

## ğŸ¯ Problem Statement

The CMCC-34 dataset suffers from severe class imbalance:
- **Most frequent class**: 1,961 samples (Class 0: å’¨è¯¢ä¸šåŠ¡è§„å®š)
- **Least frequent classes**: 1 sample each (Class 32: åŠç†é”€æˆ·/é‡å¼€, Class 33: å’¨è¯¢ç”µå•†è´§å“ä¿¡æ¯)
- **Imbalance ratio**: 1961:1
- **Gini coefficient**: 0.651 (high inequality)

## ğŸš€ Key Features

### âœ¨ Improved GLM-4 Integration
- **Enhanced API Server**: Custom GLM-4 API compatible with OpenAI format
- **Robust Error Handling**: Automatic retry mechanism with exponential backoff
- **Batch Processing**: Generate 10+ samples per API call (vs 5 previously)
- **Smart Targeting**: Class-specific sample targets based on severity

### ğŸ“Š Advanced Configuration
- **Ultra-minority Classes**: Target 300 samples for classes with <10 samples
- **Minority Classes**: Target 200 samples for classes with <50 samples
- **Adaptive Thresholds**: Dynamic minority class identification
- **Quality Control**: Length, similarity, and keyword validation

### ğŸ¨ Professional Output Organization
- **Structured Output**: All results organized in `outputs/` directory
- **Visual Analytics**: Comprehensive comparison plots and distributions
- **Detailed Reports**: Machine-readable summaries and human-readable analysis

## ğŸ“ Directory Structure

```
aug/
â”œâ”€â”€ ğŸ“Š Core Scripts
â”‚   â”œâ”€â”€ run_augmentation.py          # Main augmentation runner
â”‚   â”œâ”€â”€ visualization.py             # ğŸ†• Main visualization script
â”‚   â””â”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ âš™ï¸ Configuration
â”‚   â””â”€â”€ augment_config.yaml          # Enhanced configuration with class-specific targets
â”œâ”€â”€ ğŸ“ Data
â”‚   â””â”€â”€ train_balanced.csv           # Original balanced dataset
â”œâ”€â”€ ğŸ“ Source Code (src/)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ data_augmentation.py    # Improved GLM-4 augmentation logic
â”‚   â”‚   â””â”€â”€ convert_data.py         # Data conversion utilities
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ api_server.py           # Custom GLM-4 API server
â”‚   â”‚   â””â”€â”€ regenerate_dataset.py   # Dataset regeneration script
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ generate_report.py      # Comprehensive reporting
â”‚   â””â”€â”€ plotting/
â”‚       â”œâ”€â”€ plot_final_comparison.py    # Three-way comparison
â”‚       â”œâ”€â”€ plot_balanced_comparison.py # GLM-4 vs Original comparison
â”‚       â”œâ”€â”€ plot_simple_distribution.py # Distribution analysis
â”‚       â””â”€â”€ plot_distribution.py        # Enhanced distribution plots
â”œâ”€â”€ ğŸ“ Outputs (Generated)
â”‚   â”œâ”€â”€ visualizations/              # All visualization outputs
â”‚   â”‚   â”œâ”€â”€ balanced_distribution.png
â”‚   â”‚   â”œâ”€â”€ original_distribution.png
â”‚   â”‚   â”œâ”€â”€ before_after_comparison.png
â”‚   â”‚   â””â”€â”€ before_after_comparison_metrics.json
â”‚   â”œâ”€â”€ plots/                       # Additional plot outputs
â”‚   â”œâ”€â”€ reports/                     # Analysis reports
â”‚   â”œâ”€â”€ logs/                        # Augmentation logs
â”‚   â””â”€â”€ train_balanced.jsonl         # Generated balanced dataset
â””â”€â”€ ğŸ“š Documentation
    â””â”€â”€ README.md                    # This file
```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
# Install required packages
pip install -r requirements.txt
```

### 2. Start GLM-4 API Server
```bash
# Start the GLM-4 API server (runs on port 8001)
python src/scripts/api_server.py
```

### 3. Run Data Augmentation

```bash
# Option 1: Use the convenient launcher (recommended)
python run_augmentation.py --dry-run    # Test configuration
python run_augmentation.py              # Run full augmentation

# Option 2: Run directly from src (ensure PYTHONPATH is set)
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"
python src/scripts/run_aug.py --dry-run
python src/scripts/run_aug.py
```

### 4. Generate Visualizations and Analysis

```bash
# Generate comprehensive analysis and visualizations
python visualization.py

# Or run individual components
python src/utils/generate_report.py
python src/plotting/plot_final_comparison.py
python src/plotting/plot_balanced_comparison.py
python src/plotting/plot_simple_distribution.py
```

### 5. View Results

```bash
# Check output structure
ls -la outputs/
ls -la outputs/visualizations/

# View analysis report
cat outputs/reports/data_augmentation_report.txt

# Check generated dataset
wc -l train_balanced.jsonl
```

## ğŸ“Š Expected Results

### ğŸ¯ Augmentation Targets
| Class Type | Classes | Original Samples | Target Samples | Improvement |
|------------|---------|------------------|----------------|-------------|
| **Ultra-minority** | 32, 33, 29, 30, 31 | 1-5 each | 300 each | 60-300x |
| **Minority** | 20, 22-28 | 15-49 each | 200 each | 4-13x |

### ğŸ“ˆ Expected Improvements
- **Dataset Growth**: ~3,000+ new synthetic samples
- **Gini Coefficient**: 0.651 â†’ ~0.400 (significant improvement)
- **Imbalance Ratio**: 1961:1 â†’ ~10:1 (dramatic reduction)
- **Model Performance**: Better F1-scores on minority classes

## âš™ï¸ Configuration

### Key Parameters in `augment_config.yaml`:

```yaml
augmentation:
  target_samples_per_class: 200    # Default target per class
  batch_size: 10                   # Samples per API call
  max_retries: 5                   # API failure tolerance
  delay_between_calls: 0.5         # API rate limiting

class_specific_strategies:
  ultra_minority:
    classes: [32, 33, 29, 30, 31]
    target_samples: 300            # Aggressive augmentation
  minority:
    classes: [20, 22, 23, 24, 25, 26, 27, 28]
    target_samples: 200            # Moderate augmentation
```

## ğŸ¨ Visualization Features

### Main Visualization Script (`visualization.py`)
- **Comprehensive Analysis**: Single script for all visualization needs
- **Before/After Comparison**: Visual comparison of original vs augmented datasets
- **Distribution Analysis**: Class distribution plots and metrics
- **Quality Metrics**: Generated samples quality assessment

### Individual Plotting Scripts
1. **`plot_final_comparison.py`**: Three-way comparison showing Original â†’ GLM-4 â†’ Enhanced results
2. **`plot_balanced_comparison.py`**: Detailed GLM-4 vs Original analysis
3. **`plot_simple_distribution.py`**: Clean distribution visualization
4. **`plot_distribution.py`**: Enhanced distribution plots with detailed metrics

### Output Files
- **`balanced_distribution.png`**: Final balanced dataset distribution
- **`original_distribution.png`**: Original dataset distribution
- **`before_after_comparison.png`**: Side-by-side comparison
- **`before_after_comparison_metrics.json`**: Quantitative comparison metrics

## ğŸ”§ Technical Improvements

### vs. Original Implementation
| Aspect | Original | Improved |
|--------|----------|----------|
| **Batch Size** | 5 samples | 10 samples |
| **Retry Logic** | Basic | Exponential backoff |
| **Class Targeting** | Fixed threshold | Class-specific targets |
| **Error Handling** | Minimal | Comprehensive |
| **Output Organization** | Scattered | Structured in `outputs/` |
| **Visualization** | Basic | Comprehensive with multiple scripts |
| **Documentation** | Basic | Comprehensive |

### API Compatibility
- **Endpoint**: `http://localhost:8001/v1/chat/completions`
- **Format**: OpenAI-compatible JSON
- **Model**: GLM-4-9B-0414 (local deployment)
- **Features**: Streaming, temperature control, token limits

## ğŸ“Š Analysis Tools

### Reporting Tools
1. **`generate_report.py`**: Comprehensive statistical analysis
2. **`augmentation_summary.json`**: Machine-readable metrics
3. **Automated quality checks**: Length, similarity, keyword validation

### Quality Control
- **Length Validation**: Ensures generated samples are appropriate length
- **Similarity Check**: Prevents duplicate or very similar samples
- **Keyword Validation**: Verifies class-specific keywords are present
- **Format Consistency**: Maintains consistent JSONL format

## ğŸš¨ Troubleshooting

### Common Issues

1. **GLM-4 Server Not Starting**
   ```bash
   # Check if port 8001 is free
   netstat -tulpn | grep 8001
   
   # Kill existing process if needed
   pkill -f api_server
   ```

2. **Out of Memory**
   ```bash
   # Monitor GPU memory
   nvidia-smi
   
   # Reduce batch_size in config
   batch_size: 5  # Instead of 10
   ```

3. **API Connection Failed**
   ```bash
   # Test API health
   curl http://localhost:8001/health
   
   # Check server logs
   tail -f api_server.log
   ```

4. **Visualization Errors**
   ```bash
   # Check matplotlib backend
   python -c "import matplotlib; print(matplotlib.get_backend())"
   
   # Set backend if needed
   export MPLBACKEND=Agg
   ```

## ğŸ“ˆ Performance Monitoring

### Success Metrics
- **Generated Samples**: Target 2,000+ synthetic samples
- **Success Rate**: >80% API calls successful
- **Quality Score**: Generated samples pass validation
- **Balance Improvement**: Gini coefficient reduction >0.2

### Progress Tracking
```bash
# Monitor generation progress
tail -f outputs/logs/augmentation.log

# Check current dataset size
wc -l train_balanced.jsonl

# Analyze class distribution
python -c "
import pandas as pd
df = pd.read_csv('data/train_balanced.csv')
print(df['c_numerical'].value_counts().sort_index())
"
```

## ğŸ¯ Next Steps

1. **Train Model**: Use `train_balanced.jsonl` for model training
2. **Evaluate Performance**: Compare F1-scores on minority classes
3. **Iterate**: Adjust targets based on model performance
4. **Scale**: Apply to other imbalanced datasets

## ğŸ“š References

- **GLM-4 Model**: [THUDM/GLM-4-9B-0414](https://huggingface.co/THUDM/GLM-4-9B-0414)
- **CMCC-34 Dataset**: Telecom customer service intent classification
- **Evaluation Metrics**: Gini coefficient, F1-score, precision/recall
- **Technical Paper**: [LLM-based Data Augmentation for Imbalanced Text Classification]

---

ğŸ’¡ **Pro Tip**: Start with dry-run mode to verify configuration, then run full augmentation during off-peak hours due to computational requirements.
