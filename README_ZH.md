# GLM-4 å¾®è°ƒç”¨äº CMCC-34 æ„å›¾åˆ†ç±»

æœ¬ä»“åº“åŒ…å«ä½¿ç”¨ QLoRAï¼ˆé‡åŒ–ä½ç§©é€‚åº”ï¼‰åœ¨ CMCC-34 æ•°æ®é›†ä¸Šå¾®è°ƒ GLM-4-9B è¿›è¡Œæ„å›¾åˆ†ç±»çš„å®Œæ•´æµç¨‹ã€‚é¡¹ç›®åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œæ¨ç†åŠŸèƒ½ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè®¾ç½®

```bash
# å®‰è£…ä¾èµ–
pip install torch transformers peft bitsandbytes accelerate
pip install scikit-learn matplotlib seaborn tqdm

# æ”¯æŒ CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 2. æ•°æ®å‡†å¤‡

```bash
# å°†åŸå§‹ CSV æ•°æ®è½¬æ¢ä¸º GLM-4 æ ¼å¼ï¼ˆä½¿ç”¨ç³»ç»Ÿæç¤ºï¼‰
cd finetune/data/cmcc-34
python convert_data.py

# å¦‚éœ€é‡æ–°ç”Ÿæˆæ•°æ®é›†
python regenerate_dataset.py
```

### 3. æ¨¡å‹è®­ç»ƒ

```bash
# ä½¿ç”¨ QLoRA è®­ç»ƒæ¨¡å‹
cd finetune
python train_cmcc34_system_prompt.py
```

### 4. æ¨¡å‹è¯„ä¼°

```bash
# å¿«é€Ÿè¯„ä¼°ï¼ˆ100ä¸ªæ ·æœ¬ï¼‰
cd evaluation
python evaluate.py --quick --samples 100

# å®Œæ•´è¯„ä¼°
python evaluate.py 
```

### 5. æ¨¡å‹æ¨ç†

```bash
# äº¤äº’å¼å‘½ä»¤è¡Œç•Œé¢
cd inference
python trans_cli_finetuned_demo.py

# Web ç•Œé¢
python trans_web_finetuned_demo.py
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
GLM-4/
â”œâ”€â”€ finetune/                    # å¾®è°ƒæµç¨‹
â”‚   â”œâ”€â”€ configs/                 # è®­ç»ƒé…ç½®
â”‚   â”‚   â””â”€â”€ cmcc34_qlora_system_prompt.yaml
â”‚   â”œâ”€â”€ data/                    # æ•°æ®å¤„ç†è„šæœ¬
â”‚   â”‚   â””â”€â”€ cmcc-34/
â”‚   â”‚       â”œâ”€â”€ convert_data.py
â”‚   â”‚       â”œâ”€â”€ regenerate_dataset.py
â”‚   â”‚       â”œâ”€â”€ train.jsonl
â”‚   â”‚       â””â”€â”€ test.jsonl
â”‚   â”œâ”€â”€ output/                  # è®­ç»ƒè¾“å‡º
â”‚   â”‚   â””â”€â”€ cmcc34_qlora_system_prompt/
â”‚   â””â”€â”€ train_cmcc34_system_prompt.py
â”œâ”€â”€ evaluation/                  # æ¨¡å‹è¯„ä¼°
â”‚   â”œâ”€â”€ evaluate.py             # ä¸»è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ output/                 # è¯„ä¼°ç»“æœ
â”œâ”€â”€ inference/                   # æ¨¡å‹æ¨ç†
â”‚   â”œâ”€â”€ trans_cli_finetuned_demo.py
â”‚   â”œâ”€â”€ trans_web_finetuned_demo.py
â”‚   â””â”€â”€ test_finetuned_model.py
â”œâ”€â”€ demo/                        # æ¼”ç¤ºåº”ç”¨
â”œâ”€â”€ resources/                   # é™„åŠ èµ„æº
â””â”€â”€ README.md
```

## ğŸ”§ é…ç½®

### è®­ç»ƒé…ç½®

è®­ç»ƒä½¿ç”¨ QLoRAï¼Œä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

- **åŸºç¡€æ¨¡å‹**: GLM-4-9B-0414
- **é‡åŒ–**: 4-bit (QLoRA)
- **LoRA ç§©**: 64
- **LoRA Alpha**: 128
- **å­¦ä¹ ç‡**: 2e-4
- **æ‰¹æ¬¡å¤§å°**: 4
- **æœ€å¤§æ­¥æ•°**: 5000
- **ç³»ç»Ÿæç¤º**: é’ˆå¯¹æ„å›¾åˆ†ç±»ä¼˜åŒ–

### è¯„ä¼°é…ç½®

- **æµ‹è¯•æ•°æ®é›†**: CMCC-34 æµ‹è¯•é›†
- **æŒ‡æ ‡**: å‡†ç¡®ç‡ã€F1-å®å¹³å‡ã€F1-åŠ æƒå¹³å‡
- **è¾“å‡º**: å¤±è´¥é¢„æµ‹ã€æ··æ·†çŸ©é˜µã€è¯¦ç»†åˆ†æ
- **é‡è¯•é€»è¾‘**: è‡ªåŠ¨é‡è¯•å’Œå†…å®¹æˆªæ–­

## ğŸ“Š æ¨¡å‹æ€§èƒ½

### å¯ç”¨æ£€æŸ¥ç‚¹

`finetune/output/cmcc34_qlora_system_prompt/` ä¸­æœ‰å¤šä¸ªæ£€æŸ¥ç‚¹ï¼š

- `checkpoint-500/` - æ—©æœŸè®­ç»ƒ
- `checkpoint-1000/` - 1000 æ­¥
- `checkpoint-2000/` - 2000 æ­¥
- `checkpoint-3000/` - 3000 æ­¥
- `checkpoint-4000/` - 4000 æ­¥
- `checkpoint-5000/` - æœ€æ–°ï¼ˆæ¨èï¼‰

### æ€§èƒ½æŒ‡æ ‡

æ¨¡å‹è¾¾åˆ°ï¼š
- **å‡†ç¡®ç‡**: ~85-90%ï¼ˆå–å†³äºæ£€æŸ¥ç‚¹ï¼‰
- **F1-å®å¹³å‡**: ~0.85-0.90
- **F1-åŠ æƒå¹³å‡**: ~0.85-0.90

## ğŸ¯ æ„å›¾åˆ†ç±»

æ¨¡å‹å°†ç”¨æˆ·æ„å›¾åˆ†ç±»ä¸º 34 ä¸ªç±»åˆ«ï¼š

| æ„å›¾ID | æ„å›¾åç§° |
|--------|----------|
| 0 | å’¨è¯¢ï¼ˆå«æŸ¥è¯¢ï¼‰ä¸šåŠ¡è§„å®š |
| 1 | åŠç†å–æ¶ˆ |
| 2 | å’¨è¯¢ï¼ˆå«æŸ¥è¯¢ï¼‰ä¸šåŠ¡èµ„è´¹ |
| 3 | å’¨è¯¢ï¼ˆå«æŸ¥è¯¢ï¼‰è¥é”€æ´»åŠ¨ä¿¡æ¯ |
| 4 | å’¨è¯¢ï¼ˆå«æŸ¥è¯¢ï¼‰åŠç†æ–¹å¼ |
| 5 | æŠ•è¯‰ï¼ˆå«æŠ±æ€¨ï¼‰ä¸šåŠ¡ä½¿ç”¨é—®é¢˜ |
| 6 | å’¨è¯¢ï¼ˆå«æŸ¥è¯¢ï¼‰è´¦æˆ·ä¿¡æ¯ |
| 7 | åŠç†å¼€é€š |
| 8 | å’¨è¯¢ï¼ˆå«æŸ¥è¯¢ï¼‰ä¸šåŠ¡è®¢è´­ä¿¡æ¯æŸ¥è¯¢ |
| 9 | æŠ•è¯‰ï¼ˆå«æŠ±æ€¨ï¼‰ä¸çŸ¥æƒ…å®šåˆ¶é—®é¢˜ |
| 10 | å’¨è¯¢ï¼ˆå«æŸ¥è¯¢ï¼‰äº§å“/ä¸šåŠ¡åŠŸèƒ½ |
| ... | ... |

## ğŸ› ï¸ ä½¿ç”¨ç¤ºä¾‹

### å‘½ä»¤è¡Œè¯„ä¼°

```bash
# å¿«é€Ÿè¯„ä¼° 50 ä¸ªæ ·æœ¬
python evaluate.py --quick --samples 50

# ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„è¿›è¡Œå®Œæ•´è¯„ä¼°
python evaluate.py --model-path ../finetune/output/cmcc34_qlora_system_prompt/checkpoint-5000

# è‡ªå®šä¹‰æ‰¹æ¬¡å¤§å°å’Œè¾“å‡ºç›®å½•
python evaluate.py --batch-size 25 --output-dir my_evaluation_results
```

### ç¼–ç¨‹ä½¿ç”¨

```python
from evaluation.evaluate import SystemPromptEvaluator

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = SystemPromptEvaluator(
    base_model_path="THUDM/GLM-4-9B-0414",
    finetuned_path="finetune/output/cmcc34_qlora_system_prompt/checkpoint-5000",
    test_file="finetune/data/cmcc-34/test.jsonl",
    output_dir="evaluation_results"
)

# åŠ è½½æ¨¡å‹å¹¶è¯„ä¼°
evaluator.load_model()
test_data = evaluator.load_test_data()
results = evaluator.evaluate_batch(test_data)

# æ‰“å°ç»“æœ
evaluator.print_results(results)
evaluator.save_final_results(results)
```

### äº¤äº’å¼æ¨ç†

```bash
# å¯åŠ¨äº¤äº’å¼å‘½ä»¤è¡Œç•Œé¢
cd inference
python trans_cli_finetuned_demo.py

# ç¤ºä¾‹å¯¹è¯ï¼š
# ç”¨æˆ·: æˆ‘æƒ³æŸ¥è¯¢æˆ‘çš„ä½™é¢
# åŠ©æ‰‹: æ„å›¾ï¼š6:å’¨è¯¢ï¼ˆå«æŸ¥è¯¢ï¼‰è´¦æˆ·ä¿¡æ¯
```

## ğŸ“ˆ è¯„ä¼°ç»“æœ

è¯„ä¼°è„šæœ¬ç”Ÿæˆå…¨é¢çš„ç»“æœï¼š

### è¾“å‡ºæ–‡ä»¶

- `failed_predictions.json` - å¤±è´¥é¢„æµ‹è¯¦æƒ…
- `error_predictions.json` - é”™è¯¯é¢„æµ‹åˆ†æ
- `confusion_matrix.png` - å¯è§†åŒ–æ··æ·†çŸ©é˜µ
- `detailed_analysis.json` - æ¯ç±»æ€§èƒ½æŒ‡æ ‡
- `system_prompt_evaluation_results.json` - å®Œæ•´ç»“æœ

### åˆ†æåŠŸèƒ½

- **å¤±è´¥é¢„æµ‹åˆ†æ**: è¯¦ç»†çš„é”™è¯¯åˆ†ç±»
- **æ··æ·†çŸ©é˜µ**: åˆ†ç±»é”™è¯¯çš„å¯è§†åŒ–è¡¨ç¤º
- **æ¯ç±»æ€§èƒ½**: æ¯ä¸ªæ„å›¾çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
- **é”™è¯¯åˆ†å¸ƒ**: æœ€æ˜“æ··æ·†çš„æ„å›¾å¯¹
- **é‡è¯•é€»è¾‘**: è‡ªåŠ¨å¤„ç†é•¿è¾“å…¥å’Œå†…å­˜é”™è¯¯

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDA å†…å­˜ä¸è¶³**
   ```bash
   # å‡å°‘æ‰¹æ¬¡å¤§å°
   python evaluate.py --batch-size 10
   
   # ä½¿ç”¨è¾ƒå°çš„æ£€æŸ¥ç‚¹
   python evaluate.py --model-path checkpoint-2000
   ```

2. **æ¨¡å‹åŠ è½½é”™è¯¯**
   ```bash
   # æ£€æŸ¥æ¨¡å‹è·¯å¾„
   ls finetune/output/cmcc34_qlora_system_prompt/
   
   # éªŒè¯åŸºç¡€æ¨¡å‹
   python -c "from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('THUDM/GLM-4-9B-0414')"
   ```

3. **æ•°æ®åŠ è½½é—®é¢˜**
   ```bash
   # é‡æ–°ç”Ÿæˆæ•°æ®é›†
   cd finetune/data/cmcc-34
   python regenerate_dataset.py
   ```

### å†…å­˜è¦æ±‚

- **è®­ç»ƒ**: ~24GB GPU å†…å­˜ï¼ˆä½¿ç”¨ QLoRAï¼‰
- **è¯„ä¼°**: ~10GB GPU å†…å­˜
- **æ¨ç†**: ~10GB GPU å†…å­˜

## ğŸš€ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰è®­ç»ƒ

```bash
# ä¿®æ”¹è®­ç»ƒé…ç½®
vim finetune/configs/cmcc34_qlora_system_prompt.yaml

# ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ
python train_cmcc34_system_prompt.py --config custom_config.yaml
```

### è‡ªå®šä¹‰è¯„ä¼°

```bash
# åœ¨è‡ªå®šä¹‰æµ‹è¯•é›†ä¸Šè¯„ä¼°
python evaluate.py --test-file custom_test.jsonl

# æ¯”è¾ƒå¤šä¸ªæ£€æŸ¥ç‚¹
for checkpoint in 1000 2000 3000 4000 5000; do
    python evaluate.py --model-path checkpoint-$checkpoint --output-dir eval_$checkpoint
done
```

### ç”Ÿäº§éƒ¨ç½²

```python
# ä¸ºç”Ÿäº§ç¯å¢ƒåŠ è½½æ¨¡å‹
from inference.model_loader import load_finetuned_model

model, tokenizer = load_finetuned_model(
    base_model_path="THUDM/GLM-4-9B-0414",
    finetuned_path="finetune/output/cmcc34_qlora_system_prompt/checkpoint-5000"
)

# æ‰¹é‡æ¨ç†
def classify_intents(texts):
    results = []
    for text in texts:
        intent = model.generate_intent(text)
        results.append(intent)
    return results
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

- [GLM-4 è®ºæ–‡](https://arxiv.org/abs/2401.09602)
- [QLoRA è®ºæ–‡](https://arxiv.org/abs/2305.14314)
- [CMCC-34 æ•°æ®é›†](https://github.com/THUDM/GLM-4)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)

## ğŸ¤ è´¡çŒ®

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. è¿›è¡Œä¿®æ”¹
4. å¦‚é€‚ç”¨ï¼Œæ·»åŠ æµ‹è¯•
5. æäº¤æ‹‰å–è¯·æ±‚

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- THUDM æä¾›çš„ GLM-4 æ¨¡å‹
- å¾®è½¯æä¾›çš„ QLoRA æŠ€æœ¯
- å¼€æºç¤¾åŒºæä¾›çš„å„ç§å·¥å…·å’Œåº“